{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea2d8fb",
   "metadata": {},
   "source": [
    "# üìä Machine Learning Algorithm Comparison\n",
    "## Driver Monitoring System - Drowsiness Detection\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objective\n",
    "This notebook compares **6 popular Machine Learning algorithms** for driver state detection:\n",
    "1. **Logistic Regression**\n",
    "2. **SVM (Linear)** - Support Vector Machine with linear kernel\n",
    "3. **SVM (RBF)** - Support Vector Machine with RBF kernel\n",
    "4. **Random Forest**\n",
    "5. **XGBoost** - Extreme Gradient Boosting\n",
    "6. **K-Nearest Neighbors (KNN)**\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Contents\n",
    "- ‚úÖ Load and explore data from `face_data.csv`\n",
    "- ‚úÖ Data preprocessing: standardization, train/test split\n",
    "- ‚úÖ Train each model with GridSearchCV\n",
    "- ‚úÖ Detailed evaluation: Confusion Matrix, Classification Report\n",
    "- ‚úÖ Compare performance across all models\n",
    "- ‚úÖ Conclusions and recommendations\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ How to Use\n",
    "1. Run each cell in order from top to bottom\n",
    "2. Ensure `face_data.csv` exists in the working directory\n",
    "3. Each model is trained and evaluated independently\n",
    "4. Final comparison table will be displayed at the end\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b765c7",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Import Libraries\n",
    "\n",
    "Import all necessary libraries:\n",
    "- **pandas, numpy**: Data manipulation\n",
    "- **matplotlib, seaborn**: Visualization\n",
    "- **sklearn**: ML algorithms and evaluation tools\n",
    "- **xgboost**: XGBoost algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             accuracy_score, precision_score, recall_score, f1_score)\n",
    "\n",
    "# ML Algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"[OK] Successfully imported all libraries!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f9c2f9",
   "metadata": {},
   "source": [
    "## üìÇ Step 2: Load Data\n",
    "\n",
    "Data collected from `face_data.csv` contains:\n",
    "- **478 landmarks** from MediaPipe Face Mesh (each landmark has x, y coordinates)\n",
    "- **4 classes**: \n",
    "  - 0: Awake\n",
    "  - 1: Drowsy\n",
    "  - 2: Looking Down (Phone)\n",
    "  - 3: Microsleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbe51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "DATA_FILE = \"face_data.csv\"\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "print(f\"[OK] Loaded data from {DATA_FILE}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Number of samples: {len(df)}\")\n",
    "print(f\"Number of features: {df.shape[1] - 1} (478 landmarks * 2 = 956 features)\")\n",
    "\n",
    "# Display data distribution by class\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA DISTRIBUTION BY CLASS\")\n",
    "print(\"=\"*70)\n",
    "class_names = {0: \"Awake\", 1: \"Drowsy\", 2: \"Looking Down\", 3: \"Microsleep\"}\n",
    "for label, count in df['label'].value_counts().sort_index().items():\n",
    "    print(f\"Class {label} ({class_names[label]}): {count} samples ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Display first 5 rows\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FIRST 5 ROWS\")\n",
    "print(\"=\"*70)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c19d02",
   "metadata": {},
   "source": [
    "## üìä Step 3: Data Visualization\n",
    "\n",
    "Visualize class distribution to better understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e357c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "class_counts = df['label'].value_counts().sort_index()\n",
    "colors = ['#2ecc71', '#e74c3c', '#f39c12', '#9b59b6']\n",
    "axes[0].bar([class_names[i] for i in class_counts.index], class_counts.values, color=colors)\n",
    "axes[0].set_title('Sample Distribution by Class', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Classes')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add values on bars\n",
    "for i, v in enumerate(class_counts.values):\n",
    "    axes[0].text(i, v + 20, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(class_counts.values, labels=[class_names[i] for i in class_counts.index], \n",
    "            autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "axes[1].set_title('Class Distribution Percentage', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"[OK] Data is fairly balanced across classes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c6d11f",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 4: Data Preprocessing\n",
    "\n",
    "Preprocessing steps:\n",
    "1. **Separate features and target**: X (features) and y (labels)\n",
    "2. **Train/test split**: 80% train, 20% test with stratify to preserve class ratios\n",
    "3. **Standardization**: Use StandardScaler to scale features to mean=0, std=1\n",
    "\n",
    "**Why standardization?**\n",
    "- Algorithms like SVM, KNN, Logistic Regression are sensitive to feature scales\n",
    "- Helps models converge faster and achieve better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc04c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = df.drop('label', axis=1)\n",
    "y = df['label']\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Train/test split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n[OK] Train/test split:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n[OK] Data standardized (StandardScaler)\")\n",
    "print(f\"  Mean of X_train_scaled: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"  Std of X_train_scaled: {X_train_scaled.std():.6f}\")\n",
    "\n",
    "# Save scaler for later use\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(f\"\\n[OK] Saved scaler to scaler.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716e3567",
   "metadata": {},
   "source": [
    "---\n",
    "# ü§ñ PART 2: TRAIN AND EVALUATE ALGORITHMS\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Logistic Regression\n",
    "\n",
    "### üìö Algorithm Explanation\n",
    "**Logistic Regression** is a basic classification algorithm that uses the sigmoid function to predict probabilities.\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ Simple, easy to understand and interpret\n",
    "- ‚úÖ Fast training\n",
    "- ‚úÖ Effective with linearly separable data\n",
    "- ‚úÖ Provides probability predictions\n",
    "\n",
    "**Disadvantages:**\n",
    "- ‚ùå Not good with non-linear data\n",
    "- ‚ùå Assumes feature independence\n",
    "\n",
    "**Hyperparameters to tune:**\n",
    "- `C`: Regularization strength (smaller = stronger regularization)\n",
    "- `penalty`: L1 or L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ce0f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"1Ô∏è‚É£ LOGISTIC REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define model\n",
    "lr = LogisticRegression(max_iter=2000, random_state=42)\n",
    "\n",
    "# GridSearchCV to find best hyperparameters\n",
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']  # liblinear supports l1 and l2\n",
    "}\n",
    "\n",
    "grid_lr = GridSearchCV(lr, param_grid_lr, cv=5, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"\\n[TRAINING] Training Logistic Regression with GridSearchCV...\")\n",
    "grid_lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n[OK] Best parameters: {grid_lr.best_params_}\")\n",
    "print(f\"[OK] Best CV F1-score: {grid_lr.best_score_:.4f}\")\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_lr = grid_lr.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "prec_lr = precision_score(y_test, y_pred_lr, average='macro')\n",
    "rec_lr = recall_score(y_test, y_pred_lr, average='macro')\n",
    "f1_lr = f1_score(y_test, y_pred_lr, average='macro')\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy:  {acc_lr:.4f} ({acc_lr*100:.2f}%)\")\n",
    "print(f\"Precision: {prec_lr:.4f}\")\n",
    "print(f\"Recall:    {rec_lr:.4f}\")\n",
    "print(f\"F1-Score:  {f1_lr:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"-\"*70)\n",
    "print(classification_report(y_test, y_pred_lr, target_names=[class_names[i] for i in range(4)]))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[class_names[i] for i in range(4)],\n",
    "            yticklabels=[class_names[i] for i in range(4)])\n",
    "plt.title('Confusion Matrix - Logistic Regression', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "joblib.dump(grid_lr.best_estimator_, 'model_logistic_regression.pkl')\n",
    "print(\"\\n[OK] Saved model to model_logistic_regression.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0621ffbf",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ SVM Linear (Support Vector Machine - Linear Kernel)\n",
    "\n",
    "### üìö Algorithm Explanation\n",
    "**SVM** finds the optimal hyperplane to separate classes with maximum margin.\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ Effective with high-dimensional data\n",
    "- ‚úÖ Robust to outliers\n",
    "- ‚úÖ Linear kernel is fast and stable\n",
    "\n",
    "**Disadvantages:**\n",
    "- ‚ùå Slow training with large datasets\n",
    "- ‚ùå Sensitive to hyperparameters\n",
    "- ‚ùå Hard to interpret\n",
    "\n",
    "**Hyperparameters:**\n",
    "- `C`: Trade-off between margin and classification errors\n",
    "- `kernel='linear'`: Suitable for linearly separable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e2e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"2Ô∏è‚É£ SVM LINEAR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define model\n",
    "svm_linear = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# GridSearchCV\n",
    "param_grid_svm_linear = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid_svm_linear = GridSearchCV(svm_linear, param_grid_svm_linear, cv=5, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"\\n[TRAINING] Training SVM Linear with GridSearchCV...\")\n",
    "grid_svm_linear.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n[OK] Best parameters: {grid_svm_linear.best_params_}\")\n",
    "print(f\"[OK] Best CV F1-score: {grid_svm_linear.best_score_:.4f}\")\n",
    "\n",
    "# Prediction\n",
    "y_pred_svm_linear = grid_svm_linear.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "acc_svm_linear = accuracy_score(y_test, y_pred_svm_linear)\n",
    "prec_svm_linear = precision_score(y_test, y_pred_svm_linear, average='macro')\n",
    "rec_svm_linear = recall_score(y_test, y_pred_svm_linear, average='macro')\n",
    "f1_svm_linear = f1_score(y_test, y_pred_svm_linear, average='macro')\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy:  {acc_svm_linear:.4f} ({acc_svm_linear*100:.2f}%)\")\n",
    "print(f\"Precision: {prec_svm_linear:.4f}\")\n",
    "print(f\"Recall:    {rec_svm_linear:.4f}\")\n",
    "print(f\"F1-Score:  {f1_svm_linear:.4f}\")\n",
    "\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"-\"*70)\n",
    "print(classification_report(y_test, y_pred_svm_linear, target_names=[class_names[i] for i in range(4)]))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_svm_linear = confusion_matrix(y_test, y_pred_svm_linear)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_svm_linear, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=[class_names[i] for i in range(4)],\n",
    "            yticklabels=[class_names[i] for i in range(4)])\n",
    "plt.title('Confusion Matrix - SVM Linear', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "joblib.dump(grid_svm_linear.best_estimator_, 'model_svm_linear.pkl')\n",
    "print(\"\\n[OK] Saved model to model_svm_linear.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c1239a",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ SVM RBF (Support Vector Machine - RBF Kernel)\n",
    "\n",
    "### üìö Algorithm Explanation\n",
    "**SVM with RBF kernel** can handle non-linear data by mapping to higher-dimensional space.\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ Handles complex non-linear data\n",
    "- ‚úÖ Flexible with gamma and C parameters\n",
    "\n",
    "**Disadvantages:**\n",
    "- ‚ùå Very slow training\n",
    "- ‚ùå Easy to overfit if gamma is too large\n",
    "- ‚ùå Requires careful standardization\n",
    "\n",
    "**Hyperparameters:**\n",
    "- `C`: Regularization parameter\n",
    "- `gamma`: Defines influence of a single training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a7e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"3Ô∏è‚É£ SVM RBF\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "param_grid_svm_rbf = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "grid_svm_rbf = GridSearchCV(svm_rbf, param_grid_svm_rbf, cv=5, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"\\n[TRAINING] Training SVM RBF with GridSearchCV (may take a few minutes)...\")\n",
    "grid_svm_rbf.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n[OK] Best parameters: {grid_svm_rbf.best_params_}\")\n",
    "print(f\"[OK] Best CV F1-score: {grid_svm_rbf.best_score_:.4f}\")\n",
    "\n",
    "y_pred_svm_rbf = grid_svm_rbf.predict(X_test_scaled)\n",
    "\n",
    "acc_svm_rbf = accuracy_score(y_test, y_pred_svm_rbf)\n",
    "prec_svm_rbf = precision_score(y_test, y_pred_svm_rbf, average='macro')\n",
    "rec_svm_rbf = recall_score(y_test, y_pred_svm_rbf, average='macro')\n",
    "f1_svm_rbf = f1_score(y_test, y_pred_svm_rbf, average='macro')\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy:  {acc_svm_rbf:.4f} ({acc_svm_rbf*100:.2f}%)\")\n",
    "print(f\"Precision: {prec_svm_rbf:.4f}\")\n",
    "print(f\"Recall:    {rec_svm_rbf:.4f}\")\n",
    "print(f\"F1-Score:  {f1_svm_rbf:.4f}\")\n",
    "\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"-\"*70)\n",
    "print(classification_report(y_test, y_pred_svm_rbf, target_names=[class_names[i] for i in range(4)]))\n",
    "\n",
    "cm_svm_rbf = confusion_matrix(y_test, y_pred_svm_rbf)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_svm_rbf, annot=True, fmt='d', cmap='Oranges', \n",
    "            xticklabels=[class_names[i] for i in range(4)],\n",
    "            yticklabels=[class_names[i] for i in range(4)])\n",
    "plt.title('Confusion Matrix - SVM RBF', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "joblib.dump(grid_svm_rbf.best_estimator_, 'model_svm_rbf.pkl')\n",
    "print(\"\\n[OK] Saved model to model_svm_rbf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123b6b55",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Random Forest\n",
    "\n",
    "### üìö Algorithm Explanation\n",
    "**Random Forest** is an ensemble method that combines multiple decision trees and uses voting for final prediction.\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ High performance, robust\n",
    "- ‚úÖ Handles non-linear data well\n",
    "- ‚úÖ Less prone to overfitting\n",
    "- ‚úÖ Provides feature importance\n",
    "- ‚úÖ No need for data standardization\n",
    "\n",
    "**Disadvantages:**\n",
    "- ‚ùå Large model size, harder to deploy\n",
    "- ‚ùå Slower training with many trees\n",
    "\n",
    "**Hyperparameters:**\n",
    "- `n_estimators`: Number of trees\n",
    "- `max_depth`: Maximum depth of each tree\n",
    "- `min_samples_split`: Minimum samples required to split a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e041bd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"4Ô∏è‚É£ RANDOM FOREST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"\\n[TRAINING] Training Random Forest with GridSearchCV...\")\n",
    "grid_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n[OK] Best parameters: {grid_rf.best_params_}\")\n",
    "print(f\"[OK] Best CV F1-score: {grid_rf.best_score_:.4f}\")\n",
    "\n",
    "y_pred_rf = grid_rf.predict(X_test_scaled)\n",
    "\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "prec_rf = precision_score(y_test, y_pred_rf, average='macro')\n",
    "rec_rf = recall_score(y_test, y_pred_rf, average='macro')\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='macro')\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy:  {acc_rf:.4f} ({acc_rf*100:.2f}%)\")\n",
    "print(f\"Precision: {prec_rf:.4f}\")\n",
    "print(f\"Recall:    {rec_rf:.4f}\")\n",
    "print(f\"F1-Score:  {f1_rf:.4f}\")\n",
    "\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"-\"*70)\n",
    "print(classification_report(y_test, y_pred_rf, target_names=[class_names[i] for i in range(4)]))\n",
    "\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Purples', \n",
    "            xticklabels=[class_names[i] for i in range(4)],\n",
    "            yticklabels=[class_names[i] for i in range(4)])\n",
    "plt.title('Confusion Matrix - Random Forest', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "joblib.dump(grid_rf.best_estimator_, 'model_random_forest.pkl')\n",
    "print(\"\\n[OK] Saved model to model_random_forest.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc64c41e",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "### üìö Algorithm Explanation\n",
    "**XGBoost** is a powerful boosting algorithm that trains weak learners sequentially to improve on previous learners' errors.\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ Very high performance, often wins Kaggle competitions\n",
    "- ‚úÖ Handles complex non-linear data well\n",
    "- ‚úÖ Built-in regularization (prevents overfitting)\n",
    "- ‚úÖ Supports parallel processing\n",
    "- ‚úÖ Provides feature importance\n",
    "\n",
    "**Disadvantages:**\n",
    "- ‚ùå Many complex hyperparameters\n",
    "- ‚ùå Easy to overfit if not tuned carefully\n",
    "- ‚ùå Slower training than Random Forest\n",
    "\n",
    "**Hyperparameters:**\n",
    "- `n_estimators`: Number of boosting rounds\n",
    "- `max_depth`: Depth of trees\n",
    "- `learning_rate`: Learning rate\n",
    "- `subsample`: Fraction of samples for each tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455d629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"5Ô∏è‚É£ XGBOOST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "xgb = XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(xgb, param_grid_xgb, cv=5, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"\\n[TRAINING] Training XGBoost with GridSearchCV...\")\n",
    "grid_xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n[OK] Best parameters: {grid_xgb.best_params_}\")\n",
    "print(f\"[OK] Best CV F1-score: {grid_xgb.best_score_:.4f}\")\n",
    "\n",
    "y_pred_xgb = grid_xgb.predict(X_test_scaled)\n",
    "\n",
    "acc_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "prec_xgb = precision_score(y_test, y_pred_xgb, average='macro')\n",
    "rec_xgb = recall_score(y_test, y_pred_xgb, average='macro')\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb, average='macro')\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy:  {acc_xgb:.4f} ({acc_xgb*100:.2f}%)\")\n",
    "print(f\"Precision: {prec_xgb:.4f}\")\n",
    "print(f\"Recall:    {rec_xgb:.4f}\")\n",
    "print(f\"F1-Score:  {f1_xgb:.4f}\")\n",
    "\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"-\"*70)\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=[class_names[i] for i in range(4)]))\n",
    "\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Reds', \n",
    "            xticklabels=[class_names[i] for i in range(4)],\n",
    "            yticklabels=[class_names[i] for i in range(4)])\n",
    "plt.title('Confusion Matrix - XGBoost', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "joblib.dump(grid_xgb.best_estimator_, 'model_xgboost.pkl')\n",
    "print(\"\\n[OK] Saved model to model_xgboost.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4a3639",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ K-Nearest Neighbors (KNN)\n",
    "\n",
    "### üìö Algorithm Explanation\n",
    "**KNN** is a lazy learning algorithm that classifies based on the K nearest neighbors in feature space.\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ Simple, easy to understand\n",
    "- ‚úÖ No training required (lazy learning)\n",
    "- ‚úÖ Effective with small datasets\n",
    "\n",
    "**Disadvantages:**\n",
    "- ‚ùå Slow prediction (must compute distance to all training samples)\n",
    "- ‚ùå Sensitive to feature scales (requires standardization)\n",
    "- ‚ùå Not effective with high-dimensional data (curse of dimensionality)\n",
    "- ‚ùå Sensitive to outliers\n",
    "\n",
    "**Hyperparameters:**\n",
    "- `n_neighbors`: Number of neighbors (K)\n",
    "- `weights`: 'uniform' (all equal) or 'distance' (closer = higher weight)\n",
    "- `metric`: Distance metric (euclidean, manhattan, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd5ef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"6Ô∏è‚É£ KNN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "grid_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"\\n[TRAINING] Training KNN with GridSearchCV...\")\n",
    "grid_knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n[OK] Best parameters: {grid_knn.best_params_}\")\n",
    "print(f\"[OK] Best CV F1-score: {grid_knn.best_score_:.4f}\")\n",
    "\n",
    "y_pred_knn = grid_knn.predict(X_test_scaled)\n",
    "\n",
    "acc_knn = accuracy_score(y_test, y_pred_knn)\n",
    "prec_knn = precision_score(y_test, y_pred_knn, average='macro')\n",
    "rec_knn = recall_score(y_test, y_pred_knn, average='macro')\n",
    "f1_knn = f1_score(y_test, y_pred_knn, average='macro')\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy:  {acc_knn:.4f} ({acc_knn*100:.2f}%)\")\n",
    "print(f\"Precision: {prec_knn:.4f}\")\n",
    "print(f\"Recall:    {rec_knn:.4f}\")\n",
    "print(f\"F1-Score:  {f1_knn:.4f}\")\n",
    "\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"-\"*70)\n",
    "print(classification_report(y_test, y_pred_knn, target_names=[class_names[i] for i in range(4)]))\n",
    "\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_knn, annot=True, fmt='d', cmap='YlOrBr', \n",
    "            xticklabels=[class_names[i] for i in range(4)],\n",
    "            yticklabels=[class_names[i] for i in range(4)])\n",
    "plt.title('Confusion Matrix - KNN', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "joblib.dump(grid_knn.best_estimator_, 'model_knn.pkl')\n",
    "print(\"\\n[OK] Saved model to model_knn.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba430c02",
   "metadata": {},
   "source": [
    "---\n",
    "# üìà PART 3: COMPREHENSIVE COMPARISON\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "Compare performance of all 6 algorithms on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16acca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "results_comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'SVM Linear', 'SVM RBF', 'Random Forest', 'XGBoost', 'KNN'],\n",
    "    'Accuracy': [acc_lr, acc_svm_linear, acc_svm_rbf, acc_rf, acc_xgb, acc_knn],\n",
    "    'Precision': [prec_lr, prec_svm_linear, prec_svm_rbf, prec_rf, prec_xgb, prec_knn],\n",
    "    'Recall': [rec_lr, rec_svm_linear, rec_svm_rbf, rec_rf, rec_xgb, rec_knn],\n",
    "    'F1-Score': [f1_lr, f1_svm_linear, f1_svm_rbf, f1_rf, f1_xgb, f1_knn]\n",
    "})\n",
    "\n",
    "# Sort by F1-Score\n",
    "results_comparison = results_comparison.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE COMPARISON - SORTED BY F1-SCORE\")\n",
    "print(\"=\"*80)\n",
    "print(results_comparison.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best model\n",
    "best_model_name = results_comparison.iloc[0]['Model']\n",
    "best_f1 = results_comparison.iloc[0]['F1-Score']\n",
    "print(f\"\\nBEST MODEL: {best_model_name} (F1-Score: {best_f1:.4f})\")\n",
    "\n",
    "# Display styled table\n",
    "results_comparison_styled = results_comparison.style.background_gradient(cmap='RdYlGn', subset=['Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "results_comparison_styled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb02778c",
   "metadata": {},
   "source": [
    "## Comparison Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d1532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar charts comparing all metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors_list = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12', '#9b59b6', '#1abc9c']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    bars = ax.bar(results_comparison['Model'], results_comparison[metric], color=colors_list)\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_xticklabels(results_comparison['Model'], rotation=45, ha='right')\n",
    "    \n",
    "    # Add values on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"[OK] Comparison charts created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662119c",
   "metadata": {},
   "source": [
    "---\n",
    "# üéØ PART 4: CONCLUSIONS AND RECOMMENDATIONS\n",
    "---\n",
    "\n",
    "## Detailed Analysis\n",
    "\n",
    "### 1. Logistic Regression\n",
    "\n",
    "**Evaluation:**\n",
    "- Simple algorithm, fast training\n",
    "- Reasonably effective with linearly separable data\n",
    "- Good for baseline model\n",
    "\n",
    "**When to use:**\n",
    "- Need fast, simple model for deployment\n",
    "- Need probability predictions\n",
    "- Data is linearly separable\n",
    "\n",
    "**When NOT to use:**\n",
    "- Complex non-linear data\n",
    "- Need highest possible accuracy\n",
    "\n",
    "---\n",
    "\n",
    "### 2. SVM Linear\n",
    "\n",
    "**Evaluation:**\n",
    "- Good performance with high-dimensional data\n",
    "- Slower training than Logistic Regression\n",
    "- Robust to outliers\n",
    "\n",
    "**When to use:**\n",
    "- Large number of features (high-dimensional)\n",
    "- Need optimal margin\n",
    "- Data has clear decision boundaries\n",
    "\n",
    "**When NOT to use:**\n",
    "- Very large dataset (slow training)\n",
    "- Need interpretability\n",
    "\n",
    "---\n",
    "\n",
    "### 3. SVM RBF\n",
    "\n",
    "**Evaluation:**\n",
    "- Handles non-linear data well\n",
    "- Very slow training, sensitive to hyperparameters\n",
    "- Easy to overfit if not tuned carefully\n",
    "\n",
    "**When to use:**\n",
    "- Complex non-linear data\n",
    "- Have time for hyperparameter tuning\n",
    "\n",
    "**When NOT to use:**\n",
    "- Need fast training and prediction\n",
    "- Very large dataset\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Random Forest\n",
    "\n",
    "**Evaluation:**\n",
    "- Very good performance, robust\n",
    "- Less prone to overfitting, no need for standardization\n",
    "- Large model size, harder to deploy\n",
    "\n",
    "**When to use:**\n",
    "- Need high accuracy and stability\n",
    "- Need feature importance\n",
    "- Don't need high interpretability\n",
    "\n",
    "**When NOT to use:**\n",
    "- Need lightweight model for mobile/edge deployment\n",
    "- Need very fast real-time prediction\n",
    "\n",
    "---\n",
    "\n",
    "### 5. XGBoost\n",
    "\n",
    "**Evaluation:**\n",
    "- Usually provides highest accuracy\n",
    "- Flexible, many hyperparameters\n",
    "- Slower training than Random Forest\n",
    "\n",
    "**When to use:**\n",
    "- Need maximum accuracy\n",
    "- Complex non-linear data\n",
    "- Have time for hyperparameter tuning\n",
    "\n",
    "**When NOT to use:**\n",
    "- Need simple model\n",
    "- Limited compute resources\n",
    "\n",
    "---\n",
    "\n",
    "### 6. KNN\n",
    "\n",
    "**Evaluation:**\n",
    "- Simple, no training required\n",
    "- Slow prediction (must compute distances)\n",
    "- Not effective with high-dimensional data\n",
    "\n",
    "**When to use:**\n",
    "- Small dataset\n",
    "- Need quick baseline\n",
    "- Low-dimensional data\n",
    "\n",
    "**When NOT to use:**\n",
    "- High-dimensional data (curse of dimensionality)\n",
    "- Need fast prediction for production\n",
    "- Large dataset\n",
    "\n",
    "---\n",
    "\n",
    "## Final Summary\n",
    "\n",
    "### Recommended Models for Driver Monitoring System\n",
    "\n",
    "**TOP 3 Choices:**\n",
    "\n",
    "1. **Random Forest** - HIGHLY RECOMMENDED\n",
    "   - High accuracy, stable\n",
    "   - Not sensitive to outliers\n",
    "   - Feature importance helps understand model\n",
    "   - Saved to `model_random_forest.pkl`\n",
    "\n",
    "2. **XGBoost** - RECOMMENDED\n",
    "   - Similar accuracy to Random Forest\n",
    "   - Slower training but fast prediction\n",
    "   - Good if need optimal accuracy\n",
    "\n",
    "3. **SVM RBF** - ALTERNATIVE\n",
    "   - Good for smaller datasets\n",
    "   - Requires careful hyperparameter tuning\n",
    "\n",
    "### Future Development Directions\n",
    "\n",
    "1. **Collect more data**: Increase samples per class\n",
    "2. **Feature Engineering**: Extract features like EAR, MAR, Head Pose\n",
    "3. **Ensemble Methods**: Combine multiple models (voting, stacking)\n",
    "4. **Deep Learning**: Try Neural Networks with more data\n",
    "5. **Model Optimization**: Quantization, pruning for edge device deployment\n",
    "\n",
    "---\n",
    "\n",
    "### Deployment Notes\n",
    "\n",
    "- **Random Forest** is the best choice for production\n",
    "- Save both `scaler.pkl` and model to ensure proper standardization\n",
    "- Test model on real-world data before deployment\n",
    "- Monitor model performance over time\n",
    "\n",
    "---\n",
    "\n",
    "**COMPLETED!** This notebook provides detailed comparison of 6 ML algorithms."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
