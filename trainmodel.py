import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (accuracy_score, confusion_matrix, 
                            classification_report, precision_score, 
                            recall_score, f1_score, roc_auc_score, roc_curve)
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

# ==========================================
# 1. LOAD V√Ä KI·ªÇM TRA D·ªÆ LI·ªÜU
# ==========================================
print("‚è≥ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ 'dataset_full.csv'...")
try:
    data = pd.read_csv('dataset_full.csv')
except FileNotFoundError:
    print("‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file dataset_full.csv.")
    print("üí° H√£y ch·∫°y chongngugat.py ho·∫∑c collect_data.py tr∆∞·ªõc ƒë·ªÉ t·∫°o dataset.")
    exit()

print(f"‚úÖ T·ªïng s·ªë d√≤ng d·ªØ li·ªáu: {len(data)}")
print("\nüìä Ph√¢n b·ªë nh√£n:")
print(data['Label'].value_counts())
print(f"T·ª∑ l·ªá: {data['Label'].value_counts(normalize=True)}")

# Ki·ªÉm tra c√¢n b·∫±ng d·ªØ li·ªáu
label_counts = data['Label'].value_counts()
if len(label_counts) == 2:
    imbalance_ratio = abs(label_counts[0] - label_counts[1]) / len(data)
    if imbalance_ratio > 0.2:
        print(f"‚ö†Ô∏è C·∫¢NH B√ÅO: D·ªØ li·ªáu kh√¥ng c√¢n b·∫±ng! (Ch√™nh l·ªách: {imbalance_ratio*100:.1f}%)")
        print("   C√≥ th·ªÉ ·∫£nh h∆∞·ªüng ƒë·∫øn ƒë·ªô ch√≠nh x√°c. N√™n thu th·∫≠p th√™m d·ªØ li·ªáu.")

# Ki·ªÉm tra missing values
if data.isnull().sum().sum() > 0:
    print("‚ö†Ô∏è C·∫¢NH B√ÅO: C√≥ d·ªØ li·ªáu b·ªã thi·∫øu!")
    print(data.isnull().sum())

# ==========================================
# 2. CHU·∫®N B·ªä D·ªÆ LI·ªÜU
# ==========================================
# D√πng t·∫•t c·∫£ features: EAR, MAR, Pitch, Yaw, Roll
X = data[['EAR', 'MAR', 'Pitch', 'Yaw', 'Roll']]
y = data['Label']

# Chia train/test v·ªõi stratify ƒë·ªÉ gi·ªØ t·ª∑ l·ªá
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Chu·∫©n h√≥a d·ªØ li·ªáu (quan tr·ªçng v√¨ Pitch, Yaw, Roll c√≥ gi√° tr·ªã l·ªõn)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# L∆∞u scaler ƒë·ªÉ d√πng khi predict
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)
print("\n‚úÖ ƒê√£ l∆∞u scaler v√†o 'scaler.pkl'")

print(f"\n--- S·∫¥N S√ÄNG HU·∫§N LUY·ªÜN ---")
print(f"D·ªØ li·ªáu h·ªçc: {len(X_train)} d√≤ng")
print(f"D·ªØ li·ªáu thi: {len(X_test)} d√≤ng")

# ==========================================
# 3. ƒê·ªäNH NGHƒ®A C√ÅC MODEL
# ==========================================
models = {
    "Logistic Regression": LogisticRegression(
        class_weight='balanced', 
        max_iter=1000,
        random_state=42
    ),
    "SVM": SVC(
        kernel='rbf', 
        probability=True, 
        class_weight='balanced',
        random_state=42
    ),
    "Random Forest": RandomForestClassifier(
        n_estimators=100, 
        class_weight='balanced', 
        random_state=42,
        max_depth=10
    )
}

# ==========================================
# 4. TRAIN V√Ä ƒê√ÅNH GI√Å T·ª™NG MODEL
# ==========================================
results = {}
best_model = None
best_score = 0
best_name = ""

print("\n" + "="*60)
print("üöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN V√Ä SO S√ÅNH")
print("="*60)

for name, model in models.items():
    print(f"\n{'='*60}")
    print(f"ü§ñ MODEL: {name}")
    print(f"{'='*60}")
    
    # Train model (d√πng d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a)
    model.fit(X_train_scaled, y_train)
    
    # Predict
    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    
    # T√≠nh c√°c metrics
    acc = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    
    # ROC AUC (quan tr·ªçng cho b√†i to√°n imbalanced)
    try:
        roc_auc = roc_auc_score(y_test, y_pred_proba)
    except:
        roc_auc = 0
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    
    # L∆∞u k·∫øt qu·∫£
    results[name] = {
        'accuracy': acc,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'roc_auc': roc_auc,
        'confusion_matrix': cm,
        'model': model,
        'y_pred': y_pred,
        'y_pred_proba': y_pred_proba
    }
    
    # In k·∫øt qu·∫£
    print(f"\nüìä K·∫æT QU·∫¢:")
    print(f"  Accuracy:  {acc*100:.2f}%")
    print(f"  Precision: {precision*100:.2f}%")
    print(f"  Recall:    {recall*100:.2f}%")
    print(f"  F1-Score:  {f1*100:.2f}%")
    print(f"  ROC-AUC:   {roc_auc*100:.2f}%")
    
    print(f"\nüìâ Confusion Matrix:")
    print(f"  [{cm[0][0]:4d}  {cm[0][1]:4d}]  <- True Negative | False Positive")
    print(f"  [{cm[1][0]:4d}  {cm[1][1]:4d}]  <- False Negative | True Positive")
    print(f"\n  Gi·∫£i th√≠ch:")
    print(f"  - ƒêo√°n ƒë√∫ng T·ªânh t√°o: {cm[0][0]} d√≤ng")
    print(f"  - ƒêo√°n ƒë√∫ng Bu·ªìn ng·ªß: {cm[1][1]} d√≤ng")
    print(f"  - B√°o ƒê·ªòNG GI·∫¢ (Th·ª©c ‚Üí Ng·ªß): {cm[0][1]} d√≤ng")
    print(f"  - B·ªé S√ìT (Ng·ªß ‚Üí Th·ª©c): {cm[1][0]} d√≤ng ‚ö†Ô∏è NGUY HI·ªÇM!")
    
    # T√¨m model t·ªët nh·∫•t (d·ª±a tr√™n F1-score v√¨ quan tr·ªçng c·∫£ precision v√† recall)
    if f1 > best_score:
        best_score = f1
        best_model = model
        best_name = name

# ==========================================
# 5. SO S√ÅNH T·ªîNG QUAN
# ==========================================
print("\n" + "="*60)
print("üìä B·∫¢NG SO S√ÅNH T·ªîNG QUAN")
print("="*60)
print(f"{'Model':<20} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'ROC-AUC':<12}")
print("-"*60)

for name, result in results.items():
    print(f"{name:<20} {result['accuracy']*100:>10.2f}% {result['precision']*100:>10.2f}% "
          f"{result['recall']*100:>10.2f}% {result['f1']*100:>10.2f}% {result['roc_auc']*100:>10.2f}%")

# ==========================================
# 6. PH√ÇN T√çCH ƒêI·ªÇM M·∫†NH/ƒêI·ªÇM Y·∫æU
# ==========================================
print("\n" + "="*60)
print("üîç PH√ÇN T√çCH ƒêI·ªÇM M·∫†NH V√Ä ƒêI·ªÇM Y·∫æU")
print("="*60)

for name, result in results.items():
    print(f"\nüìå {name}:")
    
    # ƒêi·ªÉm m·∫°nh
    strengths = []
    if result['accuracy'] == max(r['accuracy'] for r in results.values()):
        strengths.append("Accuracy cao nh·∫•t")
    if result['precision'] == max(r['precision'] for r in results.values()):
        strengths.append("Precision cao nh·∫•t (√≠t b√°o ƒë·ªông gi·∫£)")
    if result['recall'] == max(r['recall'] for r in results.values()):
        strengths.append("Recall cao nh·∫•t (√≠t b·ªè s√≥t)")
    if result['f1'] == max(r['f1'] for r in results.values()):
        strengths.append("F1-Score cao nh·∫•t (c√¢n b·∫±ng t·ªët)")
    
    if strengths:
        print(f"  ‚úÖ ƒêi·ªÉm m·∫°nh: {', '.join(strengths)}")
    else:
        print(f"  ‚úÖ ƒêi·ªÉm m·∫°nh: Kh√¥ng c√≥ ƒëi·ªÉm n·ªïi tr·ªôi")
    
    # ƒêi·ªÉm y·∫øu
    weaknesses = []
    if result['recall'] < 0.8:
        weaknesses.append("Recall th·∫•p ‚Üí D·ªÖ b·ªè s√≥t tr∆∞·ªùng h·ª£p nguy hi·ªÉm")
    if result['precision'] < 0.8:
        weaknesses.append("Precision th·∫•p ‚Üí Nhi·ªÅu b√°o ƒë·ªông gi·∫£")
    if name == "Logistic Regression":
        weaknesses.append("Model ƒë∆°n gi·∫£n, c√≥ th·ªÉ kh√¥ng b·∫Øt ƒë∆∞·ª£c pattern ph·ª©c t·∫°p")
    elif name == "SVM":
        weaknesses.append("Ch·∫≠m h∆°n khi d·ªØ li·ªáu l·ªõn, kh√≥ tune hyperparameters")
    elif name == "Random Forest":
        weaknesses.append("C√≥ th·ªÉ overfit n·∫øu d·ªØ li·ªáu √≠t, t·ªën b·ªô nh·ªõ")
    
    if weaknesses:
        print(f"  ‚ö†Ô∏è ƒêi·ªÉm y·∫øu: {'; '.join(weaknesses)}")

# ==========================================
# 7. L∆ØU MODEL T·ªêT NH·∫§T
# ==========================================
print("\n" + "="*60)
print(f"üèÜ MODEL T·ªêT NH·∫§T: {best_name}")
print(f"ü•á F1-Score: {best_score*100:.2f}%")
print("="*60)

model_filename = "drowsiness_model.pkl"
with open(model_filename, 'wb') as f:
    pickle.dump(best_model, f)

print(f"‚úÖ ƒê√£ l∆∞u model v√†o '{model_filename}'")
print("‚úÖ ƒê√£ l∆∞u scaler v√†o 'scaler.pkl'")
print("\nüëâ B√¢y gi·ªù b·∫°n c√≥ th·ªÉ d√πng file n√†y ƒë·ªÉ ch·∫°y th·ª±c t·∫ø!")

# ==========================================
# 8. V·∫º BI·ªÇU ƒê·ªí SO S√ÅNH (T√πy ch·ªçn)
# ==========================================
try:
    # So s√°nh metrics
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    metrics = ['accuracy', 'precision', 'recall', 'f1']
    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    
    for idx, (metric, name) in enumerate(zip(metrics, metric_names)):
        ax = axes[idx // 2, idx % 2]
        values = [results[model][metric] for model in models.keys()]
        bars = ax.bar(models.keys(), values, color=['#3498db', '#e74c3c', '#2ecc71'])
        ax.set_ylabel('Score')
        ax.set_title(f'{name} Comparison')
        ax.set_ylim([0, 1])
        
        # Th√™m gi√° tr·ªã l√™n c·ªôt
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig('model_comparison.png', dpi=150)
    print("\n‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì so s√°nh v√†o 'model_comparison.png'")
    plt.close()
except Exception as e:
    print(f"\n‚ö†Ô∏è Kh√¥ng th·ªÉ v·∫Ω bi·ªÉu ƒë·ªì: {e}")
    print("   (C√≥ th·ªÉ do thi·∫øu matplotlib ho·∫∑c seaborn)")